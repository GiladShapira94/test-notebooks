{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "project_name = \"test-serv-with-remote\"\n",
    "# get/create a project and register the data prep and trainer function in it\n",
    "project = mlrun.get_or_create_project(\n",
    "    name=project_name, user_project=False, context=\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"serving-function\"></a>\n",
    "## Create and test the Serving Function\n",
    "\n",
    "The serving function kind is using [Nuclio](https://nuclio.io/): a high-performance serverless event \n",
    "and data processing open-source platform. With just a few lines of code we can take our model, **expose** it with an **http endpoint** and deploy it on \n",
    "**high-performance** infrastructure that can easily scale up to serve on a **production environment** with hundreds of \n",
    "thousands of requests per second. To read more about serving functions in MLRun and some cool advanced features like model routers, error handling and more, please refer to our [documentation](https://docs.mlrun.org/en/latest/serving/serving-graph.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.frameworks.lgbm import LGBMModelServer\n",
    "remote_func_name = \"predict_remote\"\n",
    "\n",
    "fn_remote = mlrun.new_function(remote_func_name,\n",
    "                               kind=\"serving\", \n",
    "                               image=\"mlrun/mlrun\", requirements=[\"lightgbm\"])\n",
    "\n",
    "\n",
    "fn_remote.add_model(\"lgbm_ny_taxi\", class_name=LGBMModelServer(None, name=\"lgbm_ny_taxi\", model_path=project.get_artifact('lgbm_ny_taxi').uri))\n",
    "remote_addr = fn_remote.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_function = project.set_function(name='serving', func='src/serving.py', image='mlrun/mlrun', kind=\"serving\", requirements=[\"lightgbm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the topology and get the graph object:\n",
    "graph = serving_function.set_topology(\"flow\", engine=\"async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.feature_store.steps import DateExtractor\n",
    "\n",
    "# Build the serving graph:\n",
    "graph.to(handler=\"add_airport_dist\", name=\"calculate_airport_distance\")\\\n",
    "     .to(handler=\"radian_conv_step\", name=\"calculate_radian_conv\")\\\n",
    "     .to(handler=\"sphere_dist_bear_step\", name=\"bearing_calculation\")\\\n",
    "     .to(handler=\"sphere_dist_step\", name=\"distance_calculation\")\\\n",
    "     .to(DateExtractor(parts=[\"hour\", \"day\", 'month', \"day_of_week\", 'year'],timestamp_col=\"pickup_datetime\"))\\\n",
    "     .to(handler=\"preprocess\", name=\"preprocess\")\\\n",
    "     .to(\"$remote\", remote_func_name, url=f'{remote_addr}v2/models/lgbm_ny_taxi/infer', method='put')\\\n",
    "     .to(handler=\"postprocess\", name=\"postprocess\").respond()\n",
    "\n",
    "# Plot to graph:\n",
    "graph.plot(rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mock server (in memory simulator for the graph for testing)\n",
    "server = serving_function.to_mock_server()\n",
    "\n",
    "body = {'pickup_longitude':-73.844311, 'pickup_latitude':40.721319,\n",
    "        'dropoff_longitude':-73.84161, 'dropoff_latitude': 40.712278,\n",
    "        'passenger_count':1,\n",
    "        'pickup_datetime': '2013-01-01T12', 'key': '2013-01-01T12'}\n",
    "\n",
    "# simulate a user request and print the results\n",
    "response_mock = server.test(path=\"/v2/models/lgbm_ny_taxi/infer\", body=body.copy())\n",
    "print(response_mock['result_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy-serving\"></a>\n",
    "## Deploy the serving Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy it:\n",
    "deployment = project.deploy_function(serving_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = deployment.function.invoke(path='/v2/models/lgbm_ny_taxi/infer', body=body.copy())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert response['result_str'] == response_mock['result_str']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
